# -*- coding: utf-8 -*-
"""Cote d'Ivore MTN analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18tBglEgBbjzHklnTetoOOGTsxz0I_xvA

# Anaysing the Cote d'Ivore MTN problem
In this project, we will use the CRISP-DM process to anayse the Cote d'Ivore MTN problem.

## Business Understanding
The CRISP-DM process starts with the understanding of the business problem. In this project we try to answer to the following  business questions:
* Which region is most used for the 3 days
* City that is used the most during business hours?
* How frequently was each product used in the dataset?
* What are the busiest times of the day in terms of user traffic [This should be key in identifying the most suitable times for upgrades]

## Data Understanding
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# dataset1 = pd.read_csv(r"/Telcom_dataset.csv")
description1 = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Data/CDR_description.xlsx", sheet_name=None)
description2 = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Data/cells_geo_description.xlsx", sheet_name=None)

description1

"""Now we see that our data contains the correct column names. Here is a detailed description of the first dataset, tht contains the citiesdataset columns as described in the dataset CRD_description.
 
 VILLES - City - String
 STATUS - In Service or not - String
 LOCALISATION - in ABIDJAN or not  String
 DECOUPZONE - Geographical Zone - String
 ZONENAME - Name of Zone - String
 LONGITUDE - Longitude - Float
 LATITUDE - Latitude - Float
 REGION - Region - String
 AREA -  Area - String
 CELL_ID -  ID of the cell - String
 SITE_CODE - Site - (there are several cells per site, severa...  String}

For the second dataset with th products, here is a description of the columns.

 0          PRODUCT                                       Voice or SMS   String
 1            VALUE                                      Billing price  Integer
 2        DATE_TIME               Time in format yyyy-MM-dd hh:mm:ss.0   String
 3     CELL_ON_SITE  Which cell in the site was used (not needed here)  Integer
 4  DW_A_NUMBER_INT  Anonymized phone number of the person for whic...   String
 5  DW_B_NUMBER_INT        Anonymized phone number of the counterparty   String
 6        COUNTRY_A                  Country of party A (useless here)   String
 7        COUNTRY_B                  Country of party B (useless here)   String
 8          CELL_ID                                     ID of the cell   String
 9          SITE_ID                                     ID of the SITE   String}

## Data Preparation

**Dataset 1** -- 
The geographic data 
look for missiong values and fill thom
"""

cells_geo = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Data/cells_geo.csv")
cells_geo

# cells_geo.fillna(cells_geo.mean(), inplace=True)

# # Updating the column names so that they are all correctly spelled
# # Writing the column names in all lowercase
# # Changing the column name VILLES to CITY for dataset 1

cells_geo.columns = ['index', 'city', 'status', 'localisation', 'decoupzone', 'zonename', 'longitude',
              'latitude', 'region', 'area', 'cell_id', 'site_id']
cells_geo.head(4)

"""**Dataset 2**

We have three csv files with samimilar data, we can merge this into one file for easier analysis 
"""

# read the three csv files 
dataset1 = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Data/Telcom_dataset.csv")
dataset2 = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Data/Telcom_dataset2_2.csv")
dataset3 = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Data/Telcom_dataset3.csv")

# Updating the column names for the last 3 DataFrames to ensure uniformity which could come in handy when merging DataFrames
# Also changing the DATE_TIME column name to TIME which is a more accurate description of the content of that column
# This data cleaning step also corrects column misspellings in the original dataset to ensure that the attribute name represents the content of that attribute
# Finally we need to specify the null values

dataset1.columns = ['product', 'value', 'time', 'cell_on_site', 'dw_a_number', 'dw_b_number', 'country_a',
              'country_b', 'cell_id', 'site_id']
dataset1.loc[dataset1['cell_id'] == '#NAME?','cell_id'] = np.nan
dataset1.loc[dataset1['site_id'] == '#NAME?','site_id'] = np.nan
dataset1.head(4)

dataset2.columns = ['product', 'value', 'time', 'cell_on_site', 'dw_a_number', 'dw_b_number', 'country_a',
              'country_b', 'cell_id', 'site_id']
dataset2.loc[dataset2['cell_id'] == '#NAME?','cell_id'] = np.nan
dataset2.loc[dataset2['site_id'] == '#NAME?','site_id'] = np.nan
dataset2.head(4)

dataset3.columns = ['product', 'value', 'time', 'cell_on_site', 'dw_a_number', 'dw_b_number', 'country_a',
              'country_b', 'cell_id', 'site_id']
dataset3.loc[dataset3['cell_id'] == '#NAME?','cell_id'] = np.nan
dataset3.loc[dataset3['site_id'] == '#NAME?','site_id'] = np.nan

dataset3.head(4)

"""Combine the three datasets into one"""

# Combining all three DatatFrames 7,8 and 9
# Since the have the same attributes, structure and data content
telcom_dataset = pd.concat([dataset1, dataset2, dataset3])
telcom_dataset

"""# Splitting the date and time column"""

# to split the date and time column into separate columns

telcom_dataset.head()

# create a new column and solit the date_time column into 2

telcom_dataset[['date', 'time']] = telcom_dataset.time.str.split(expand=True)
telcom_dataset.head()

"""# Dropping unnecessary columns"""

# Dropping unnecessary columns 

telcom_dataset = telcom_dataset.drop(columns = ['country_a' ,	'country_b' ,'cell_on_site'])
telcom_dataset.head()

"""Merge the data inn the telcom dataset with the cells_geo dataset"""

# Join the concatenated DataFrame df with the cells_geo DataFrame i.e df1
# I will use an inner join to make this merge
# First I will drop the cell id column on the cells_geo DataFrame
# This is because there are several cell ids per site but we are only intrested in the cell ids
# that have other relevant data like value and product which is containted in the concatenated table above
# I also deleted duplicates
cells_geo = cells_geo.drop(columns=['cell_id','index'])
cells_geo
cells_geo_b = cells_geo.drop_duplicates()
cells_geo_final = cells_geo_b[cells_geo_b['status'] == 'In Service']
cells_geo_final

df_all = pd.merge(telcom_dataset, cells_geo_final, how='inner', on='site_id')
df_all.tail(4)



"""### Data Analysis

Most used city

First, find the most used SITE_ID from the telcom dataset and then compare with the SITE_ID in the cells_geo dataset
"""

# We can also use the count operation to count the number of times
# each city appered in the consolidated dataset
# After executing the code below, we can see that YOPOUGON was the most used city

df_all.city.value_counts()

"""**City that is used the most during business hours**
Assuming business hours is from 7am to 5 pm, select cells whose date time is between 7am and 5pm
"""

#slice according to datetime
# telcom_dataset_datetime = telcom_dataset.set_index('DATE_TIME')['2009-05-01' :'2010-03-01']
# print(telcom_dataset_datetime)

telcom_dataset["time"] = pd.to_datetime(telcom_dataset["time"])
telcom_dataset["time"]

"""**How frequently was each product used in the dataset**"""

# Here I am using the group by and count function 
# on a subset of the DataFrame in previous question

df_price = df_all[['product', 'date', 'value']]
df_price.groupby([ df_price['product']]).count()

"""**What was the total billing price for each product in the dataset?**"""

# Here I am using the group by and sum aggregate function

df_price.groupby(['product', 'date']).sum()

"""**Which localisation by product were the most used in the dataset?**"""

# We can respond to this question using the group by and  count function

df_region = df_all[['region', 'localisation', 'product']]
df_region.groupby(['localisation', 'product']).count()

"""# Getting the region that has most users.

"""

#
region = pd.value_counts(df_all['region'])
print(f"\nThe most used region in 3 days are:\n")
print(f"Region               Count\n\
-------------------------\n{region.head()}")

"""# Explore product usage in high traffic cities"""

import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

product_usage = plt.figure(figsize=(8,8))
usage = df_all['product'].value_counts()
sns.barplot(x= usage, y=usage.index)
plt.xlabel('Product Usage')
plt.savefig('usage.png')
files.download('usage.png')

"""# SMS services are more frequently used"""

services = df_all['product'].value_counts(normalize=True)
services

"""#Which product brings in more money?"""

profits = df_all.groupby(['product'])['value'].sum()
(profits / profits.sum())*100

"""# Group by value"""

df_all.groupby(['city'])['value'].sum().sort_values(ascending=False)

"""# Which city brings in more money per product?

"""

product_bills = df_all.groupby(['city','product'])['value'].sum().reset_index()
product_bills

"""# Recomendations

Results
*   Voice is the most popular product
*   'ffa6759bb2' is the busiest cell. Expansion on this cell needed

Region to Expand

After analysis, the following cities were identified to have the highest traffic based on available data and should therefore be the first cities to be considered in carrying out the upgrade plan:

* YOPOUGON
* ABOBO
*COCODY
*ADJAME
*KOUMASSI
*MARCORY
*YAMOUSSOUKRO

After comparison, the sms product was found to have a higher frequency of usage than the voice product. SMS occupying 61.4% while Voice occupies 38.5% of the total traffic. However, it is important to note that voice brings in 81.2% of the total profits while sms only brings in 18.7%.

Among the high traffic cities, the following 5 cities bring in the highest profits, from both products:

* YOPOUGON
* COCODY
* ABOBO
* KOUMASSI
* TREICHVILLE
"""